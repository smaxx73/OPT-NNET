{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, IFrame\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron et theorie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un perceptron linéaire envoie une entrée $x \\in \\mathbb{R}^n$ ($n$ values) to an output which can be a binary output $F(x) \\in \\{0,1\\}$ (1 value which is one or zero).\n",
    "\n",
    "This function is decomposed in two parts : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **linear function** : defined by $n$ weights $a_1,...,a_n$ :\n",
    "$$f(x) = a_1x_1 + a_2x_2 +...+ a_n x_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example : $n=2$, $(a_1,a_2) = (2,3)$ so that $f(x_1,x_2) = 2x_1 + 3x_2$ and $f(4,-1) = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([4,-1])  #n-vector x\n",
    "a = np.array([2,3])   #n weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,a):\n",
    "    return sum(x*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f(x,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **activation function** : it is a function $\\varphi \\colon \\mathbb{R} \\to \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example : Heaviside function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(y):\n",
    "    if y<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H(f(x,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.linspace(-6,6,300)\n",
    "z=np.array([H(t) for t in y])\n",
    "plt.plot(y,z,linewidth=7.0,color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is the number $H(f(x)) \\in \\{0,1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example : ReLu function\n",
    "\n",
    "The Rectified Linear Unit function is the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLu(y):\n",
    "    if y<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.linspace(-6,6,300)\n",
    "z=np.array([ReLu(t) for t in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y,z,linewidth=7.0,color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example : Sigmoïd function\n",
    "\n",
    "The Sigmoïd function is the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(y):\n",
    "    return 1/(1+np.exp(-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.linspace(-6,6,300)\n",
    "z=np.array([sigmoid(t) for t in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y,z,linewidth=7.0,color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise : compute the derivative of the sigmoïd function and plot its graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsigmoid(y):\n",
    "    return np.exp(-y)/(1+np.exp(-y))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.linspace(-6,6,300)\n",
    "z=np.array([sigmoid(t) for t in y])\n",
    "dz=np.array([dsigmoid(t) for t in y])\n",
    "#plt.plot(y,z,linewidth=7.0,color=\"red\")\n",
    "plt.plot(y,dz,linewidth=4.0,color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear pereceptron is represented with a **neuron** as follows : <img src=\"Perceptron1.png\"> </img> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n=2$, $(a_1,a_2) = (2,3)$ so that $f(x_1,x_2) = 2x_1 + 3x_2$ and the activation function Heaviside : the output is\n",
    "\n",
    "- 1 if $2x_1+3x_2 \\geq 0$ ;\n",
    "- 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IFrame('https://www.geogebra.org/calculator/x63nersc?embed',900,400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "How to find a perceptron separating blue circles from red squares ?\n",
    "https://www.geogebra.org/calculator/zmyaznwf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IFrame('https://www.geogebra.org/calculator/zmyaznwf?embed',600,400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a biais $a_0$ : this a $(n+1)$-th weight which defines an affine function $f(x_1,...,x_n) = a_1x_1+...+a_nx_n + a_0$. An affine pereceptron is represented by a neuron as follows :\n",
    "<img src=\"Perceptron2.png\"> </img> \n",
    "\n",
    "Example : with 2 entries, an affine perceptron split a 2-dimensional space into 2 half planes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or, and, xor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer science, a boolean variable is a variable $x$ that has 1 of 2 possible values (TRUE or FALSE). In a boolean algebra, if $x$ et $y$ are boolean, we can define ```x OR y```.\n",
    "\n",
    "We chose a graphic representation : TRUE is number 1, FALSE is number 0, ```x OR y``` is a point with $(x,y)$ coordinates in plane. This point is a red square if ```x OR y = TRUE```, a blue circle otherwise.\n",
    "\n",
    "<img src=\"img/or_perceptron1.png\"> </img> \n",
    "\n",
    "1. Can I realize this operation ```x OR y``` with a perceptron ?\n",
    "\n",
    "<img src=\"img/or_perceptron2.png\"> </img> \n",
    "\n",
    "Yes ! For example, take the weights $(a_1,a_2,a_0) = (1,1,-1)$. \n",
    "\n",
    "2. In the same way, can I realize the operation ```x AND y``` with a perceptron ?\n",
    "\n",
    "<img src=\"img/and_perceptron1.png\"> </img> \n",
    "\n",
    "Yes ! For example, take the weights $(a_1,a_2,a_0) = (1,1,-1.5)$. \n",
    "\n",
    "3. In the same way, can I realize the operation ```x XOR y``` with a perceptron ?\n",
    "\n",
    "<img src=\"img/xor_perceptron.png\"> </img> \n",
    "\n",
    "No ! You cannot find a straight line that separates these two kind of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is to know that the two sets of points are **linearly separable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "In an $n$-dimensionnal Euclidian space, two sets of points $A$ and $B$ are linearly separable if an hyperplane can separate space : there exists $a_1,...,a_n,a_0$ such that for each $x \\in A$, $\\sum_{i=1}^n a_i x_i +a_0> 0$ and for each $x \\in B$, $\\sum_{i=1}^n a_i x_i + a_0 < 0$.     \n",
    "    An another way to say that is their respective convex hulls are disjoint.\n",
    " </div>\n",
    " \n",
    " <div class=\"alert alert-danger\" role=\"alert\">\t\n",
    "\n",
    "In an $n$-dimensionnal Euclidian space, two sets of points $A$ and $B$ are linearly separable if there exists a perceptron that takes value 1 on $A$ and 0 on $B$.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron is a **linear classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "\n",
    "Is it possible to realize the operation ```x OR y OR z``` with a perceptron ?\n",
    "\n",
    "<img src=\"img/or_or_perceptron.png\"> </img> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wow, your perceptron is learning for the first time !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This learning rule is an example of supervised training, in which the learning rule is provided\n",
    "with a set of examples of proper perecptron behavior: a collection of $(x,t)$ where $x$ is an input and $t$ is a the corresponding target output. As each input is applied to the network, the network output is compared\n",
    "to the target. **The learning rule then adjusts the weights and biases\n",
    "of the perceptron in order to move the perceptron output closer to the target.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test problem\n",
    "These are 3  input/target pairs for our test problem :\n",
    "$$x_1 = (1,2) \\,;\\, t_1 = 1 \\qquad x_2 = (-1,2) \\,;\\, t_2 = 0 \\qquad x_3 = (0,-1) \\,;\\, t_3 = 0$$\n",
    "\n",
    "The perceptron for this problem should have two-inputs and one output. To\n",
    "simplify our development of the learning rule, we will begin with a network\n",
    "without a bias so that we are looking for two weights $a_1,a_2$. Activation function is Heaviside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[];t=[]\n",
    "x.append(np.array([1,2])) ; t.append(1)\n",
    "x.append(np.array([-1,2])) ; t.append(0)\n",
    "x.append(np.array([0,-1])) ; t.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing Learning Rules\n",
    "We set the weight vector $w = (a_1,a_2)$ to the following randomly generated values: $w = (1.0, -0.8)$. \n",
    "\n",
    "Then we execute the perceptron with the first input $x_1$ : output $y_1$ is equal to $0 \\neq t_1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1.0,-0.8])\n",
    "y = sum(w*x[0])\n",
    "print(y)\n",
    "y = H(y)\n",
    "print(y)\n",
    "y == t[0] #test if output y_1 is equal to t_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule** : if $t=1$ and $y=0$ then $w_{new} := w_{old} + x$.\n",
    "\n",
    "Then we execute the perceptron with the second input $x_2$ and new weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w + x[0]\n",
    "y = H(sum(w*x[1]))\n",
    "y == t[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule** : if $t=0$ and $y=1$ then $w_{new} := w_{old} - x$.\n",
    "\n",
    "Then we execute the perceptron with the second input $x_3$ and new weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w - x[1]\n",
    "y = H(sum(w*x[2]))\n",
    "y == t[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y - t[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the previous rule :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w - x[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we check :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    y = H(sum(w*x[i]))\n",
    "    print(y == t[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfying Learning Rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rule can be extended to train the bias by noting that a bias is simply\n",
    "a weight whose input is always 1.\n",
    "\n",
    "#### Exercise :\n",
    "Write a program that applies all the rules given above with different weights at start. Check the result.\n",
    "\n",
    "Bonus : Represent with a 2d-graph the evolution of $w$ by plotting the corresponding linear classifyer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = len(x)\n",
    "w = np.array([-0.5,-0.9])\n",
    "for i in range(n):\n",
    "    y = H(sum(w*x[i]))\n",
    "    err = t[i]-y\n",
    "    w = w + err * x[i]\n",
    "for i in range(3):\n",
    "    y = H(sum(w*x[i]))\n",
    "    print(y == t[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
